{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
    "\n",
    "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
    "* it can incorporate prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
    "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
    "\n",
    "In this homework you will implement this ratio for a dataset of your choice to characterize the words that differentiate each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first job is to find two datasets with some interesting opposition -- e.g., news articles from CNN vs. FoxNews, books written by Charles Dickens vs. James Joyce, screenplays of dramas vs. comedies.  Be creative -- this should be driven by what interests you and should reflect your own originality. **This dataset cannot come from Kaggle**.  Feel feel to use web scraping (see [here](https://github.com/CU-ITSS/Web-Data-Scraping-S2023) for a great tutorial) or manually copying/pasting text.  Aim for more than 10,000 tokens for each dataset. \n",
    "   \n",
    "Save those datasets in two files: \"class1_dataset.txt\" and \"class2_dataset.txt\" \n",
    "\n",
    "Q1. Describe each of those datasets and their source in 100-200 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two dataset I used are both transcripts or subtitles from Youtube videos. Because txt files are not supported in uploading of homework, the api and script used to download the transcripts are kept below.\n",
    "\n",
    "The first dataset is the combined transcripts of OverSimplified's two videos of American Civil War (part I and part II). This Youtuber has so far posted only 29 videos in the past 7 years but gained 7.8 million subscribers, making him an oddly popular content creater under the history video topic. His videos tend to be amusing. There are always some funny details around the history event in his videos. (With the total duration of 53 minutes, as 30 & 23, the two videos together produce 9721 tokens before removing stopwords)\n",
    "\n",
    "The second dataset is the transcript of a single long video of almost 90 minutes from Youtuber WarsofTheWorld. His video narration tends to be traditional like the ones on TV shows. As of now, he has 291 thousands of subscribers, which is a decent number but not comparable to OverSimplified. His video about American Civil War is relatively detailed and historically accurate about the events in this historic era.  (With the total duration of 90 minutes, the video produces 14281 tokens before removing stopwords)\n",
    "\n",
    "Considering the difference in their narration style (and neglecting the animations and sound effects used in the video for this study), I would like to know specificially what are their preferences of word choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\linzh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "OS_text_list_1 = YouTubeTranscriptApi.get_transcript(\"tsxmyL7TUJg&t=1s\")\n",
    "OS_text_list_2 = YouTubeTranscriptApi.get_transcript(\"sV6uuMAnJUE&t=410s\")\n",
    "OS_text_list = OS_text_list_1 + OS_text_list_2\n",
    "\n",
    "WTW_text_list = YouTubeTranscriptApi.get_transcript(\"38KcVu5DkhA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Tokenize those texts by filling out the `read_and_tokenize` function below (your choice of tokenizer). The input is a filename and the output should be a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(filename):\n",
    "    \n",
    "    lower_str = ' '.join([filename[i]['text'] for i in range(len(filename))]).lower()\n",
    "    no_punc_str = ''.join([character for character in lower_str if character not in punctuation])\n",
    "    no_digit_str = ''.join([character for character in no_punc_str if not character.isdigit()])\n",
    "    \n",
    "    orig_tokens = word_tokenize(no_digit_str)\n",
    "    \n",
    "    # tokens = [word for word in orig_tokens if not word in stop_words]     # remove stopwords only\n",
    "    tokens = [porter.stem(word) for word in orig_tokens if not word in stop_words]     # remove stopwords & remove stemmer\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these file paths to wherever the datasets you created above live.\n",
    "class1_tokens=read_and_tokenize(OS_text_list)\n",
    "class2_tokens=read_and_tokenize(WTW_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5387, 8048)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class1_tokens), len(class2_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.  Now let's find the words that characterize each of those sources (with respect to the other). Implement the log-odds ratio with an uninformative Dirichlet prior.  This value, $\\hat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\hat\\zeta_w^{(i-j)}= {\\hat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$\n",
    "\\hat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
    "* $\\alpha_w$ = 0.01\n",
    "* $V$ = size of vocabulary (number of distinct word types)\n",
    "* $\\alpha_0 = V * \\alpha_w$\n",
    "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
    "\n",
    "In this example, the two corpora are your class1 dataset (e.g., $i$ = your class1) and your class2 dataset (e.g., $j$ = class2). Using this metric, print out the 25 words most strongly aligned with class1, and 25 words most strongly aligned with class2.  Again, consult [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import math\n",
    "\n",
    "def find_freq_tokens(tokens, display_limit):\n",
    "    \n",
    "    freq_dict = FreqDist(tokens)\n",
    "    bigram_freq = dict()\n",
    "    \n",
    "    sorted_items = sorted(freq_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_keys = [item[0] for item in sorted_items[:display_limit]]\n",
    "    \n",
    "    return top_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_in_usage(word, one_tokens, two_tokens):\n",
    "    \n",
    "    freq_dict_1 = FreqDist(one_tokens)\n",
    "    freq_dict_2 = FreqDist(two_tokens)\n",
    "    \n",
    "    # prepare variables for the equation\n",
    "    y_i_w = freq_dict_1[word]\n",
    "    y_j_w = freq_dict_2[word]\n",
    "    alpha_w = 0.01\n",
    "    n_i = len(one_tokens)\n",
    "    n_j = len(one_tokens)\n",
    "    alpha_0 = len(freq_dict_1 + freq_dict_2) * alpha_w\n",
    "    \n",
    "    # calculation\n",
    "    numerator = (math.log((y_i_w + alpha_w) / (n_i + alpha_0 - y_i_w - alpha_w)) - \n",
    "                 math.log((y_j_w + alpha_w) / (n_j + alpha_0 - y_j_w - alpha_w)))\n",
    "    denominator = math.sqrt(1 / (y_i_w + alpha_w) + 1 / (y_j_w + alpha_w))\n",
    "    \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds_with_uninformative_prior(one_tokens, two_tokens, display=25):\n",
    "    \n",
    "    one_top_words = find_freq_tokens(one_tokens, display)\n",
    "    two_top_words = find_freq_tokens(two_tokens, display)\n",
    "    \n",
    "    top_words_diff_one = dict()\n",
    "    top_words_diff_two = dict()\n",
    "    \n",
    "    for word in one_top_words:\n",
    "        top_words_diff_one[word] = difference_in_usage(word, one_tokens, two_tokens)\n",
    "    \n",
    "    for word in two_top_words:\n",
    "        top_words_diff_two[word] = difference_in_usage(word, two_tokens, one_tokens)\n",
    "    \n",
    "    return top_words_diff_one, top_words_diff_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ranking_1, token_ranking_2 = logodds_with_uninformative_prior(class1_tokens, class2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lincoln': 3.5179561549133496,\n",
       " 'confeder': -7.477058298601654,\n",
       " 'gener': -0.9145471516186507,\n",
       " 'would': -3.9235732891828667,\n",
       " 'war': -2.939859356705888,\n",
       " 'union': -9.627840823795536,\n",
       " 'north': 0.43457451767808747,\n",
       " 'one': 2.0102779549050367,\n",
       " 'lee': -1.3947309376396697,\n",
       " 'grant': 1.6499086108057,\n",
       " 'men': -0.11331830472200768,\n",
       " 'state': -4.4775747905513485,\n",
       " 'south': -1.1989778333401264,\n",
       " 'forc': -6.2635521855988125,\n",
       " 'mcclellan': 3.5192958774958165,\n",
       " 'like': 3.425984944536229,\n",
       " 'armi': -2.350182855776382,\n",
       " 'time': 0.7021736575593186,\n",
       " 'slave': -0.2639289403597094,\n",
       " 'take': 2.1722811703089446,\n",
       " 'move': 3.5491082928484605,\n",
       " 'could': 1.742666082606499,\n",
       " 'new': -0.8187224012919115,\n",
       " 'well': 2.3322916543789005,\n",
       " 'presid': -0.14346736553132017}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ranking_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keys_1 = [item for item in token_ranking_1.keys()]\n",
    "top_vals_1 = [item for item in token_ranking_1.values()]\n",
    "\n",
    "top_keys_2 = [item for item in token_ranking_2.keys()]\n",
    "top_vals_2 = [item for item in token_ranking_2.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_class1</th>\n",
       "      <th>value_class1</th>\n",
       "      <th>word_class2</th>\n",
       "      <th>value_class2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lincoln</td>\n",
       "      <td>3.517956</td>\n",
       "      <td>union</td>\n",
       "      <td>9.559404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>confeder</td>\n",
       "      <td>-7.477058</td>\n",
       "      <td>confeder</td>\n",
       "      <td>7.424273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gener</td>\n",
       "      <td>-0.914547</td>\n",
       "      <td>forc</td>\n",
       "      <td>6.237067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would</td>\n",
       "      <td>-3.923573</td>\n",
       "      <td>would</td>\n",
       "      <td>3.906376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>war</td>\n",
       "      <td>-2.939859</td>\n",
       "      <td>state</td>\n",
       "      <td>4.460704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>union</td>\n",
       "      <td>-9.627841</td>\n",
       "      <td>war</td>\n",
       "      <td>2.928033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>north</td>\n",
       "      <td>0.434575</td>\n",
       "      <td>th</td>\n",
       "      <td>5.827023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>one</td>\n",
       "      <td>2.010278</td>\n",
       "      <td>gener</td>\n",
       "      <td>0.911134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lee</td>\n",
       "      <td>-1.394731</td>\n",
       "      <td>battl</td>\n",
       "      <td>3.928163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>grant</td>\n",
       "      <td>1.649909</td>\n",
       "      <td>lee</td>\n",
       "      <td>1.390402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>men</td>\n",
       "      <td>-0.113318</td>\n",
       "      <td>attack</td>\n",
       "      <td>5.100593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>state</td>\n",
       "      <td>-4.477575</td>\n",
       "      <td>troop</td>\n",
       "      <td>4.994961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>south</td>\n",
       "      <td>-1.198978</td>\n",
       "      <td>armi</td>\n",
       "      <td>2.344640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>forc</td>\n",
       "      <td>-6.263552</td>\n",
       "      <td>south</td>\n",
       "      <td>1.195877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mcclellan</td>\n",
       "      <td>3.519296</td>\n",
       "      <td>river</td>\n",
       "      <td>4.468933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>like</td>\n",
       "      <td>3.425985</td>\n",
       "      <td>north</td>\n",
       "      <td>-0.433432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>armi</td>\n",
       "      <td>-2.350183</td>\n",
       "      <td>lincoln</td>\n",
       "      <td>-3.505366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time</td>\n",
       "      <td>0.702174</td>\n",
       "      <td>day</td>\n",
       "      <td>3.530503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>slave</td>\n",
       "      <td>-0.263929</td>\n",
       "      <td>men</td>\n",
       "      <td>0.113045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>take</td>\n",
       "      <td>2.172281</td>\n",
       "      <td>citi</td>\n",
       "      <td>2.722157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>move</td>\n",
       "      <td>3.549108</td>\n",
       "      <td>side</td>\n",
       "      <td>2.474938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>could</td>\n",
       "      <td>1.742666</td>\n",
       "      <td>american</td>\n",
       "      <td>3.207930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>new</td>\n",
       "      <td>-0.818722</td>\n",
       "      <td>command</td>\n",
       "      <td>4.056067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>well</td>\n",
       "      <td>2.332292</td>\n",
       "      <td>slaveri</td>\n",
       "      <td>1.452569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>presid</td>\n",
       "      <td>-0.143467</td>\n",
       "      <td>howev</td>\n",
       "      <td>3.008096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_class1  value_class1 word_class2  value_class2\n",
       "0      lincoln      3.517956       union      9.559404\n",
       "1     confeder     -7.477058    confeder      7.424273\n",
       "2        gener     -0.914547        forc      6.237067\n",
       "3        would     -3.923573       would      3.906376\n",
       "4          war     -2.939859       state      4.460704\n",
       "5        union     -9.627841         war      2.928033\n",
       "6        north      0.434575          th      5.827023\n",
       "7          one      2.010278       gener      0.911134\n",
       "8          lee     -1.394731       battl      3.928163\n",
       "9        grant      1.649909         lee      1.390402\n",
       "10         men     -0.113318      attack      5.100593\n",
       "11       state     -4.477575       troop      4.994961\n",
       "12       south     -1.198978        armi      2.344640\n",
       "13        forc     -6.263552       south      1.195877\n",
       "14   mcclellan      3.519296       river      4.468933\n",
       "15        like      3.425985       north     -0.433432\n",
       "16        armi     -2.350183     lincoln     -3.505366\n",
       "17        time      0.702174         day      3.530503\n",
       "18       slave     -0.263929         men      0.113045\n",
       "19        take      2.172281        citi      2.722157\n",
       "20        move      3.549108        side      2.474938\n",
       "21       could      1.742666    american      3.207930\n",
       "22         new     -0.818722     command      4.056067\n",
       "23        well      2.332292     slaveri      1.452569\n",
       "24      presid     -0.143467       howev      3.008096"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(list(zip(top_keys_1, top_vals_1, top_keys_2, top_vals_2)),\n",
    "                    columns =['word_class1', 'value_class1', 'word_class2', 'value_class2'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
