{"cells":[{"cell_type":"markdown","id":"5792cb96","metadata":{"id":"5792cb96"},"source":["In this notebook, we'll explore the basics of token representations in BERT and use it to find token nearest neighbors.  You should open this notebook in Google Colab, or use smaller BERT models locally (as in previous notebooks).\n"]},{"cell_type":"code","execution_count":1,"id":"tYjrwwbFaKWW","metadata":{"id":"tYjrwwbFaKWW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697069151338,"user_tz":420,"elapsed":11186,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"49239521-8329-4ac7-b7ab-3c7223f3bd2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":2,"id":"eddc388c","metadata":{"id":"eddc388c","executionInfo":{"status":"ok","timestamp":1697069154957,"user_tz":420,"elapsed":3630,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["from transformers import BertModel, BertTokenizer\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"id":"88a12638","metadata":{"id":"88a12638","executionInfo":{"status":"ok","timestamp":1697069157094,"user_tz":420,"elapsed":2144,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","id":"82f34541","metadata":{"id":"82f34541"},"source":["BERT uses WordPiece tokenization, which breaks down words that don't appear within its 30K-word vocabulary into small pieces.  The word \"vaccinated\", for instanced, is tokenized as [\"va\", \"##cci\", \"##nated\"]"]},{"cell_type":"code","execution_count":4,"id":"b2a1e451","metadata":{"id":"b2a1e451","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697069157095,"user_tz":420,"elapsed":15,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"286f6b25-c8ca-48bc-9141-539f41eab367"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'new',\n"," 'data',\n"," 'shows',\n"," '26',\n"," 'states',\n"," 'have',\n"," 'fully',\n"," 'va',\n"," '##cci',\n"," '##nated',\n"," 'more',\n"," 'than',\n"," 'half',\n"," 'their',\n"," 'residents',\n"," '.',\n"," '[SEP]']"]},"metadata":{},"execution_count":4}],"source":["inputs=tokenizer(\"New data shows 26 states have fully vaccinated more than half their residents.\", return_tensors=\"pt\")\n","tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"]},{"cell_type":"code","execution_count":5,"id":"a390bb2d","metadata":{"id":"a390bb2d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697069157095,"user_tz":420,"elapsed":11,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"35ba24b0-2e0f-4467-82c9-cc6e2a26c94f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'bert',\n"," 'is',\n"," 'super',\n"," '##cal',\n"," '##if',\n"," '##rag',\n"," '##ilis',\n"," '##tic',\n"," '##ex',\n"," '##pia',\n"," '##lid',\n"," '##oc',\n"," '##ious',\n"," '[SEP]']"]},"metadata":{},"execution_count":5}],"source":["inputs=tokenizer(\"BERT is supercalifragilisticexpialidocious\", return_tensors=\"pt\")\n","tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"]},{"cell_type":"markdown","id":"b058b2be","metadata":{"id":"b058b2be"},"source":["As mentioned in class, notice how every sentence input to BERT is wrapped in two tags: a start [CLS] tag and an ending [SEP] tag.  BERT will generate representations of each WordPiece token, including these special [CLS] and [SEP] tags."]},{"cell_type":"markdown","id":"e4a9c2fa","metadata":{"id":"e4a9c2fa"},"source":["To generate representations for the input tokens, simply pass the input through the BERT model:"]},{"cell_type":"code","execution_count":6,"id":"e2bc73aa","metadata":{"id":"e2bc73aa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697069157373,"user_tz":420,"elapsed":285,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"87efaf97-6a40-43aa-e100-cec8f6e53f1c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', 'this', 'jam', 'is', 'delicious', '[SEP]']"]},"metadata":{},"execution_count":6}],"source":["inputs=tokenizer(\"This jam is delicious\", return_tensors=\"pt\")\n","outputs = model(**inputs)\n","tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"]},{"cell_type":"markdown","id":"bbe3f309","metadata":{"id":"bbe3f309"},"source":["Representations for each of BERT layers (12 in this model) are accessible here, but let's explore just the outputs from the final layer.  This BERT model has 768-dimensional representations, so this 6-token input ([CLS, this, jam, is, delicious, [SEP]) has an output that is is a 1 x 6 tokens x 768 dimensional tensor."]},{"cell_type":"code","execution_count":7,"id":"967d8d59","metadata":{"id":"967d8d59","executionInfo":{"status":"ok","timestamp":1697069157373,"user_tz":420,"elapsed":14,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["last_hidden_states = outputs.last_hidden_state"]},{"cell_type":"code","execution_count":8,"id":"6fe68cc0","metadata":{"id":"6fe68cc0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697069157374,"user_tz":420,"elapsed":15,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"55823d75-7959-44b1-98e1-8c24557be5f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 6, 768])\n"]}],"source":["print(outputs.last_hidden_state.shape)"]},{"cell_type":"markdown","id":"c888e87d","metadata":{"id":"c888e87d"},"source":["What can we do with just these representations?  While we used word2vec-style static embeddings to find nearest neighbors for word *types*, we can do the same here for word *tokens*."]},{"cell_type":"code","execution_count":9,"id":"6eccbb32","metadata":{"id":"6eccbb32","executionInfo":{"status":"ok","timestamp":1697069157374,"user_tz":420,"elapsed":13,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"]},{"cell_type":"code","execution_count":10,"id":"0b79b4ed","metadata":{"id":"0b79b4ed","executionInfo":{"status":"ok","timestamp":1697069157374,"user_tz":420,"elapsed":12,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["query=\"I ate some jam with toast\""]},{"cell_type":"code","execution_count":11,"id":"cbc6477e","metadata":{"id":"cbc6477e","executionInfo":{"status":"ok","timestamp":1697069157374,"user_tz":420,"elapsed":12,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["comp_sents=[\"She got me out of a real jam\", \"This jam is made of strawberries\", \"I sat in a traffic jam for 2 hours\", \"The Grateful Dead used to jam for like two days straight.\", \"My grandma makes the best jam.\", \"I had to jam on the brakes to avoid hitting him.\"]"]},{"cell_type":"code","execution_count":12,"id":"be898d38","metadata":{"id":"be898d38","executionInfo":{"status":"ok","timestamp":1697069157476,"user_tz":420,"elapsed":113,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["def get_bert_for_token(string, term):\n","\n","    # tokenize\n","    inputs = tokenizer(string, return_tensors=\"pt\")\n","\n","    # convert input ids to words\n","    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n","\n","    # find the first location of the query term among those tokens (so we know which BERT rep to use)\n","    term_idx=tokens.index(term)\n","\n","    outputs = model(**inputs)\n","\n","    # return the BERT rep for that token index\n","    # The output is a pytorch tensor object, but let's convert it to a numpy object to work with numpy functions\n","\n","    return outputs.last_hidden_state[0][term_idx].detach().numpy()\n","\n"]},{"cell_type":"code","execution_count":13,"id":"719345c7","metadata":{"id":"719345c7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697069157476,"user_tz":420,"elapsed":8,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"18ac2692-f610-4381-a3d1-89b79ceb6847"},"outputs":[{"output_type":"stream","name":"stdout","text":["(768,)\n"]}],"source":["query_rep=get_bert_for_token(query, \"jam\")\n","print(query_rep.shape)"]},{"cell_type":"code","execution_count":14,"id":"ddd0e1e8","metadata":{"id":"ddd0e1e8","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697069158539,"user_tz":420,"elapsed":1069,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"f001c99d-5d01-4b51-8a94-330ab103c3a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.736\tI ate some jam with toast\tMy grandma makes the best jam.\n","0.665\tI ate some jam with toast\tThis jam is made of strawberries\n","0.480\tI ate some jam with toast\tI sat in a traffic jam for 2 hours\n","0.443\tI ate some jam with toast\tThe Grateful Dead used to jam for like two days straight.\n","0.385\tI ate some jam with toast\tShe got me out of a real jam\n","0.290\tI ate some jam with toast\tI had to jam on the brakes to avoid hitting him.\n"]}],"source":["vals=[]\n","for sent in comp_sents:\n","    comp_rep=get_bert_for_token(sent, \"jam\")\n","    cos_sim=cosine_similarity(query_rep, comp_rep)\n","    vals.append((cos_sim, query, sent))\n","\n","for c, q, s in reversed(sorted(vals)):\n","    print(\"%.3f\\t%s\\t%s\" % (c, q, s))"]},{"cell_type":"markdown","id":"af7167cc","metadata":{"id":"af7167cc"},"source":["**Q**: Brainstorm the variety of things you can do with token-level word representations like this and we'll discuss them at the end of class.  As one example, adapt the code above to find *any* word that is most similar to *jam* in \"I ate some jam with toast\" in the following sentences.  Are you able to find token-level synonyms this way?"]},{"cell_type":"code","execution_count":15,"id":"9294232d","metadata":{"id":"9294232d","executionInfo":{"status":"ok","timestamp":1697069158540,"user_tz":420,"elapsed":11,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["comp_sents=[\"My grandma makes the best jelly.\", \"Jelly is made of strawberries\"]"]},{"cell_type":"code","source":["query=\"I ate some jam with toast\""],"metadata":{"id":"Bt-awOH50mWO","executionInfo":{"status":"ok","timestamp":1697069158540,"user_tz":420,"elapsed":9,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"id":"Bt-awOH50mWO","execution_count":16,"outputs":[]},{"cell_type":"code","source":["query_rep=get_bert_for_token(query, \"jam\")\n","print(query_rep.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DuH12ZFg0h0D","executionInfo":{"status":"ok","timestamp":1697069158745,"user_tz":420,"elapsed":213,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"cee06cd5-9a3d-443b-bf04-fe8daf839aed"},"id":"DuH12ZFg0h0D","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["(768,)\n"]}]},{"cell_type":"code","execution_count":19,"id":"42395734","metadata":{"id":"42395734","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697069174631,"user_tz":420,"elapsed":3215,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"ab6778c4-dd53-43c1-9b4a-f423ebc7b6a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.512\tI ate some jam with toast\tMy grandma makes the best jelly.\tjelly\n","0.394\tI ate some jam with toast\tJelly is made of strawberries\tjelly\n","0.380\tI ate some jam with toast\tJelly is made of strawberries\t##berries\n","0.378\tI ate some jam with toast\tJelly is made of strawberries\tmade\n","0.376\tI ate some jam with toast\tJelly is made of strawberries\tstraw\n","0.370\tI ate some jam with toast\tMy grandma makes the best jelly.\tbest\n","0.360\tI ate some jam with toast\tMy grandma makes the best jelly.\tthe\n","0.341\tI ate some jam with toast\tJelly is made of strawberries\tof\n","0.336\tI ate some jam with toast\tJelly is made of strawberries\tis\n","0.324\tI ate some jam with toast\tMy grandma makes the best jelly.\tmakes\n","0.215\tI ate some jam with toast\tMy grandma makes the best jelly.\tgrandma\n","0.197\tI ate some jam with toast\tMy grandma makes the best jelly.\t.\n","0.166\tI ate some jam with toast\tMy grandma makes the best jelly.\tmy\n","0.150\tI ate some jam with toast\tJelly is made of strawberries\t[CLS]\n","0.128\tI ate some jam with toast\tMy grandma makes the best jelly.\t[CLS]\n","0.045\tI ate some jam with toast\tMy grandma makes the best jelly.\t[SEP]\n","0.035\tI ate some jam with toast\tJelly is made of strawberries\t[SEP]\n"]}],"source":["vals=[]\n","for sent in comp_sents:\n","    inputs=tokenizer(sent, return_tensors=\"pt\")\n","    words = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n","    for word in words:\n","        comp_rep=get_bert_for_token(sent, word)\n","        cos_sim=cosine_similarity(query_rep, comp_rep)\n","        vals.append((cos_sim, query, sent, word))\n","\n","for c, q, s, w in reversed(sorted(vals)):\n","    print(\"%.3f\\t%s\\t%s\\t%s\" % (c, q, s, w))"]},{"cell_type":"code","source":[],"metadata":{"id":"tQU_zehYMPzX","executionInfo":{"status":"aborted","timestamp":1697069159250,"user_tz":420,"elapsed":8,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"id":"tQU_zehYMPzX","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":5}