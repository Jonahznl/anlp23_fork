{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Thie notebook explores using transformers for document classification.  Before starting, change the runtime to GPU: Runtime > Change runtime type > Hardware accelerator: GPU (any GPU is fine).\n","\n","For an intro to models in pytorch, see [this tutorial](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html).\n","\n","\n"],"metadata":{"id":"ubRzhaCVXYUy"}},{"cell_type":"markdown","source":["Download classification data for training/evaluation."],"metadata":{"id":"eQnqbL-NjmiP"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"X9UUcu7TG6sg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696825663763,"user_tz":420,"elapsed":213,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"29e90790-d0ac-47db-8e15-a44eb36579c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-09 04:27:43--  https://raw.githubusercontent.com/dbamman/anlp23/main/data/convote/train.tsv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4660140 (4.4M) [text/plain]\n","Saving to: ‘train.tsv.2’\n","\n","\rtrain.tsv.2           0%[                    ]       0  --.-KB/s               \rtrain.tsv.2         100%[===================>]   4.44M  --.-KB/s    in 0.05s   \n","\n","2023-10-09 04:27:43 (82.6 MB/s) - ‘train.tsv.2’ saved [4660140/4660140]\n","\n","--2023-10-09 04:27:43--  https://raw.githubusercontent.com/dbamman/anlp23/main/data/convote/dev.tsv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 351382 (343K) [text/plain]\n","Saving to: ‘dev.tsv.2’\n","\n","dev.tsv.2           100%[===================>] 343.15K  --.-KB/s    in 0.03s   \n","\n","2023-10-09 04:27:43 (12.1 MB/s) - ‘dev.tsv.2’ saved [351382/351382]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/dbamman/anlp23/main/data/convote/train.tsv\n","!wget https://raw.githubusercontent.com/dbamman/anlp23/main/data/convote/dev.tsv"]},{"cell_type":"code","source":["import math\n","import sys\n","import torch\n","from torch import nn\n","from collections import Counter\n","from nltk import word_tokenize"],"metadata":{"id":"PwsQCWgmHLrP","executionInfo":{"status":"ok","timestamp":1696825667286,"user_tz":420,"elapsed":3531,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"id":"eS2rympQIObz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696825667560,"user_tz":420,"elapsed":277,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"8c45aa0e-5195-4487-d0ff-9269eb4e18d9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"metadata":{"id":"QDlR2HlLHO7J","executionInfo":{"status":"ok","timestamp":1696825667560,"user_tz":420,"elapsed":3,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# max sequence length\n","max_length=256\n","\n","# limit vocabulary to top N words in training data\n","max_vocab=10000\n","\n","# batch size\n","batch_size=128\n","\n","# size of token representations (which dictates the size of the overall model).\n","d_model=16\n","\n","\n","# number of epochs\n","num_epochs=50\n","\n","print('')\n","print(\"********************************************\")\n","print(\"Running on: {}\".format(device))\n","print(\"********************************************\")\n","print('')"],"metadata":{"id":"l3zDewVZHW23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696825667560,"user_tz":420,"elapsed":3,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"75962bf1-cefb-475f-cd90-9d50ee969eb7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","********************************************\n","Running on: cuda\n","********************************************\n","\n"]}]},{"cell_type":"code","source":["# PositionalEncoding class copied from:\n","# https://github.com/pytorch/examples/blob/main/word_language_model/model.py\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_length, d_model)\n","        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)#.transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n"],"metadata":{"id":"h5IJ2unzHYzu","executionInfo":{"status":"ok","timestamp":1696825667560,"user_tz":420,"elapsed":2,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class TransformerClassifier(torch.nn.Module):\n","\n","    def __init__(self, num_labels, d_model, nhead=2, num_encoder_layers=1, dim_feedforward=256):\n","\n","        super(TransformerClassifier, self).__init__()\n","\n","        self.num_labels=num_labels\n","        self.embedding = nn.Embedding(num_embeddings=max_vocab+2, embedding_dim=d_model)\n","        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward, batch_first=True)\n","        self.classifier = nn.Linear(d_model, self.num_labels)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","\n","    def forward(self, x, m):\n","\n","        # put data on device (e.g., gpu)\n","        x=x.to(device)\n","        m=m.to(device)\n","\n","        # convert input token IDs to word embeddings\n","        embed=self.embedding(x)\n","\n","        # add position encodings to include information about word position within the document\n","        embed = self.pos_encoder(embed)\n","\n","        # get transformer output\n","        h=self.transformer.encoder(embed, src_key_padding_mask=m)\n","\n","        # Represent document as average embedding of transformer output\n","        h=torch.mean(h, dim=1)\n","\n","        # Convert document representation into output label space\n","        logits=self.classifier(h)\n","\n","        return logits\n"],"metadata":{"id":"pMTmsPDjHast","executionInfo":{"status":"ok","timestamp":1696825667560,"user_tz":420,"elapsed":2,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def create_vocab_and_labels(filename, max_vocab):\n","    # This function creates the word vocabulary (and label ids) from the training data\n","    # The vocab is a mapping between word types and unique word IDs\n","\n","    counts=Counter()\n","    labels={}\n","    with open(filename, encoding=\"utf-8\") as file:\n","        for line in file:\n","            cols=line.rstrip().split(\"\\t\")\n","            lab=cols[0]\n","            text=word_tokenize(cols[1].lower())\n","            for tok in text:\n","                counts[tok]+=1\n","\n","            if lab not in labels:\n","                labels[lab]=len(labels)\n","\n","    vocab={\"[MASK]\":0, \"[UNK]\":1}\n","\n","    for k,v in counts.most_common(max_vocab):\n","        vocab[k]=len(vocab)\n","\n","    return vocab, labels"],"metadata":{"id":"TT56POLcHcLM","executionInfo":{"status":"ok","timestamp":1696825667560,"user_tz":420,"elapsed":1,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def read_data(filename, vocab, labels, max_length, max_docs=5000):\n","    # Read in data from file, up to the first max_docs documents. For each document\n","    # read up to max_length tokens.\n","\n","    x=[]\n","    y=[]\n","    m=[]\n","\n","    with open(filename, encoding=\"utf-8\") as file:\n","        for idx, line in enumerate(file):\n","            if idx >= max_docs:\n","                break\n","            cols=line.rstrip().split(\"\\t\")\n","            lab=cols[0]\n","            text=word_tokenize(cols[1])\n","            text_ids=[]\n","            for tok in text:\n","                if tok in vocab:\n","                    text_ids.append(vocab[tok])\n","                else:\n","                    text_ids.append(vocab[\"[UNK]\"])\n","\n","            text_ids=text_ids[:max_length]\n","\n","            # PyTorch (and most libraries that deal with matrix operations) expects all inputs to be the same length\n","            # So pad each document with 0s up to max_length\n","            # But keep track of the true number of tokens in the document with the \"mask\" list.\n","\n","            # True tokens have a mask value of 0\n","            mask=[0]*len(text_ids)\n","\n","            for i in range(len(text_ids), max_length):\n","                text_ids.append(vocab[\"[MASK]\"])\n","                # Padded tokens have a mask value of 1\n","                mask.append(1)\n","\n","            x.append(text_ids)\n","            m.append(mask)\n","            y.append(labels[lab])\n","\n","    return x, y, m"],"metadata":{"id":"tB9Vv3TiHdkW","executionInfo":{"status":"ok","timestamp":1696825667765,"user_tz":420,"elapsed":206,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def get_batches(x, y, m, batch_size):\n","\n","    # Create minibatches from the full dataset\n","\n","    batches_x=[]\n","    batches_y=[]\n","    batches_m=[]\n","    for i in range(0, len(x), batch_size):\n","        xbatch=x[i:i+batch_size]\n","        ybatch=y[i:i+batch_size]\n","        mbatch=m[i:i+batch_size]\n","\n","        batches_x.append(torch.LongTensor(xbatch))\n","        batches_y.append(torch.LongTensor(ybatch))\n","        batches_m.append(torch.BoolTensor(mbatch))\n","\n","    return batches_x, batches_y, batches_m"],"metadata":{"id":"dglYDfx-HfEt","executionInfo":{"status":"ok","timestamp":1696825667766,"user_tz":420,"elapsed":2,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, all_x, all_y, all_m):\n","\n","    # Calculate accuracy\n","\n","    model.eval()\n","    corr = 0.\n","    total = 0.\n","    with torch.no_grad():\n","        for x, y, m in zip(all_x, all_y, all_m):\n","            y_preds=model.forward(x, m)\n","            for idx, y_pred in enumerate(y_preds):\n","                prediction=torch.argmax(y_pred)\n","                if prediction == y[idx]:\n","                    corr += 1.\n","                total+=1\n","    return corr/total"],"metadata":{"id":"_g5m_NjzHgsW","executionInfo":{"status":"ok","timestamp":1696825667766,"user_tz":420,"elapsed":2,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train(model, model_filename, train_batches_x, train_batches_y, train_batches_m, dev_batches_x, dev_batches_y, dev_batches_m):\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    cross_entropy=nn.CrossEntropyLoss()\n","\n","    # Keep track of the epoch that has the best dev accuracy\n","    best_dev_acc=0.\n","    best_dev_epoch=None\n","\n","    # How many epochs with no changes before we quit\n","    patience=10\n","\n","    for epoch in range(num_epochs):\n","\n","        model.train()\n","\n","        for x, y, m in zip(train_batches_x, train_batches_y, train_batches_m):\n","            # Get predictions for batch x (with mask values m)\n","            y_pred=model.forward(x, m)\n","            y=y.to(device)\n","\n","            # Calculate loss as cross-entropy with true labels\n","            loss = cross_entropy(y_pred.view(-1, model.num_labels), y.view(-1))\n","\n","            # Set all gradients to zero\n","            optimizer.zero_grad()\n","\n","            # Calculate gradients from current loss\n","            loss.backward()\n","\n","            # Update parameters\n","            optimizer.step()\n","\n","        dev_accuracy=evaluate(model, dev_batches_x, dev_batches_y, dev_batches_m)\n","\n","        # we're going to save the model that performs the best on *dev* data\n","        if dev_accuracy > best_dev_acc:\n","            torch.save(model.state_dict(), model_filename)\n","            print(\"%.3f is better than %.3f, saving model ...\" % (dev_accuracy, best_dev_acc))\n","            best_dev_acc = dev_accuracy\n","            best_dev_epoch=epoch\n","\n","        if epoch % 1 == 0:\n","            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n","\n","        if epoch-best_dev_epoch > patience:\n","          print(\"%s > patience (%s), stopping...\" % (epoch-best_dev_epoch, patience))\n","          break\n","\n","    model.load_state_dict(torch.load(model_filename))\n","    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))"],"metadata":{"id":"eLjdIDl4HiDE","executionInfo":{"status":"ok","timestamp":1696825667766,"user_tz":420,"elapsed":2,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["vocab, labels=create_vocab_and_labels(\"train.tsv\", max_vocab)\n","train_x, train_y, train_m=read_data(\"train.tsv\", vocab, labels, max_length=max_length)\n","dev_x, dev_y, dev_m=read_data(\"dev.tsv\", vocab, labels, max_length=max_length)"],"metadata":{"id":"_nkNh6f8HkL2","executionInfo":{"status":"ok","timestamp":1696825683042,"user_tz":420,"elapsed":15278,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"igxZuO5WtOuH","executionInfo":{"status":"ok","timestamp":1696825683042,"user_tz":420,"elapsed":4,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"a06cf007-4bfa-4d62-c17f-df5d8249ff89"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'D': 0, 'R': 1}"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["classifier=TransformerClassifier(num_labels=len(labels), d_model=100, dim_feedforward=1024)\n","classifier=classifier.to(device)\n","\n","train_x_batch, train_y_match, train_m_match=get_batches(train_x, train_y, train_m, batch_size=batch_size)\n","dev_x_batch, dev_y_match, dev_m_match=get_batches(dev_x, dev_y, dev_m, batch_size=batch_size)\n","\n","train(classifier, \"test.model\", train_x_batch, train_y_match, train_m_match, dev_x_batch, dev_y_match, dev_m_match)"],"metadata":{"id":"0vtxV6jpHrAV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696825705395,"user_tz":420,"elapsed":22355,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"162413de-1694-4629-986f-9ed09de1a12d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:296: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n","  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"]},{"output_type":"stream","name":"stdout","text":["0.502 is better than 0.000, saving model ...\n","Epoch 0, dev accuracy: 0.502\n","0.525 is better than 0.502, saving model ...\n","Epoch 1, dev accuracy: 0.525\n","Epoch 2, dev accuracy: 0.521\n","0.533 is better than 0.525, saving model ...\n","Epoch 3, dev accuracy: 0.533\n","0.553 is better than 0.533, saving model ...\n","Epoch 4, dev accuracy: 0.553\n","0.572 is better than 0.553, saving model ...\n","Epoch 5, dev accuracy: 0.572\n","0.611 is better than 0.572, saving model ...\n","Epoch 6, dev accuracy: 0.611\n","0.626 is better than 0.611, saving model ...\n","Epoch 7, dev accuracy: 0.626\n","0.646 is better than 0.626, saving model ...\n","Epoch 8, dev accuracy: 0.646\n","Epoch 9, dev accuracy: 0.642\n","0.654 is better than 0.646, saving model ...\n","Epoch 10, dev accuracy: 0.654\n","0.665 is better than 0.654, saving model ...\n","Epoch 11, dev accuracy: 0.665\n","Epoch 12, dev accuracy: 0.654\n","Epoch 13, dev accuracy: 0.654\n","Epoch 14, dev accuracy: 0.642\n","Epoch 15, dev accuracy: 0.634\n","Epoch 16, dev accuracy: 0.580\n","Epoch 17, dev accuracy: 0.654\n","Epoch 18, dev accuracy: 0.665\n","Epoch 19, dev accuracy: 0.626\n","Epoch 20, dev accuracy: 0.626\n","Epoch 21, dev accuracy: 0.630\n","Epoch 22, dev accuracy: 0.615\n","11 > patience (10), stopping...\n","\n","Best Performing Model achieves dev accuracy of : 0.665\n"]}]},{"cell_type":"markdown","source":["**Q1**. Play around with this transformer as implemented and experiment with how performance on the dev data changes as a function of `d_model`, `num_encoder_layers`, `nhead`, etc.).  Describe your experiments and report dev accuracy on them below."],"metadata":{"id":"R3dn2o1KKdz_"}},{"cell_type":"markdown","source":["Default parameters in class TransformerClassifier:\\\n","d_model, nhead=2, num_encoder_layers=1, dim_feedforward=256 \\\n","Results with...\\\n","Pre-set param (d_model=100, nhead=2, num_encoder_layers=1, dim_feedforward=1024): 0.623, 0.638, 0.630"],"metadata":{"id":"1K2BROWv-4or"}},{"cell_type":"markdown","source":["Results with...\\\n","Param set 1 (d_model=100, nhead=2, num_encoder_layers=1, dim_feedforward=256): 0.696, 0.693,  0.634\\\n","Param set 2 (d_model=100, nhead=2, num_encoder_layers=2, dim_feedforward=1024): 0.654, 0.689, 0.638 \\\n","Param set 3 (d_model=100, nhead=2, num_encoder_layers=1, dim_feedforward=4096): 0.658, 0.623, 0.642 \\\n","Param set 4 (d_model=400, nhead=8, num_encoder_layers=4, dim_feedforward=4096): 0.525, 0.525, 0.494 \\\n","Increasing numbers for d_model, n_head, num_encoder_layers, and dim_feedforward does not necessarily guarantee an increase in accuracy."],"metadata":{"id":"lQibUex4Vfnc"}},{"cell_type":"code","source":["classifier=TransformerClassifier(num_labels=len(labels), d_model=400, nhead=8, num_encoder_layers=4, dim_feedforward=4096)\n","classifier=classifier.to(device)\n","\n","train_x_batch, train_y_match, train_m_match=get_batches(train_x, train_y, train_m, batch_size=batch_size)\n","dev_x_batch, dev_y_match, dev_m_match=get_batches(dev_x, dev_y, dev_m, batch_size=batch_size)\n","\n","train(classifier, \"test.model\", train_x_batch, train_y_match, train_m_match, dev_x_batch, dev_y_match, dev_m_match)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7QP8ARK4jzP","executionInfo":{"status":"ok","timestamp":1696826053906,"user_tz":420,"elapsed":348546,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"193c13ee-4273-4cbe-ac97-2b4ec9245c0f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["0.494 is better than 0.000, saving model ...\n","Epoch 0, dev accuracy: 0.494\n","Epoch 1, dev accuracy: 0.494\n","Epoch 2, dev accuracy: 0.494\n","Epoch 3, dev accuracy: 0.494\n","Epoch 4, dev accuracy: 0.494\n","Epoch 5, dev accuracy: 0.494\n","Epoch 6, dev accuracy: 0.494\n","Epoch 7, dev accuracy: 0.494\n","Epoch 8, dev accuracy: 0.494\n","Epoch 9, dev accuracy: 0.494\n","Epoch 10, dev accuracy: 0.494\n","Epoch 11, dev accuracy: 0.494\n","11 > patience (10), stopping...\n","\n","Best Performing Model achieves dev accuracy of : 0.494\n"]}]},{"cell_type":"markdown","source":["**Q2**.  This transformer is forced to learn everything about the structure of language from the labeled dataset.  Word embeddings, however, already capture some of this structure, and can be incorporated into this model in an `nn.Embedding` layer.  Change the `TransformerClassifier` class above so that the `Embedding` layer uses pre-trained weights (do so with the `Embedding.from_pretrained` function described on the pytorch [API](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).  You can use any pre-trained embeddings you like, including the [GloVe vectors](https://raw.githubusercontent.com/dbamman/anlp23/main/data/glove.6B.50d.50K.txt) from class.  (Hint: doing so will require changes to `read_data` and `create_vocab_and_labels` since the word embeddings will give you your vocabulary.)"],"metadata":{"id":"6tSj2tAMI88r"}},{"cell_type":"code","source":["class NewTransformerClassifier(torch.nn.Module):\n","\n","    def __init__(self, num_labels, d_model, weight_list, nhead=2, num_encoder_layers=1, dim_feedforward=256):\n","\n","        super(NewTransformerClassifier, self).__init__()\n","\n","        self.num_labels=num_labels\n","        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weight_list))\n","        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward, batch_first=True)\n","        self.classifier = nn.Linear(d_model, self.num_labels)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","\n","    def forward(self, x, m):\n","\n","        # put data on device (e.g., gpu)\n","        x=x.to(device)\n","        m=m.to(device)\n","\n","        # convert input token IDs to word embeddings\n","        embed=self.embedding(x)\n","\n","        # add position encodings to include information about word position within the document\n","        embed = self.pos_encoder(embed)\n","\n","        # get transformer output\n","        h=self.transformer.encoder(embed, src_key_padding_mask=m)\n","\n","        # Represent document as average embedding of transformer output\n","        h=torch.mean(h, dim=1)\n","\n","        # Convert document representation into output label space\n","        logits=self.classifier(h)\n","\n","        return logits\n"],"metadata":{"id":"XiWT6yumKD_l","executionInfo":{"status":"ok","timestamp":1696826053907,"user_tz":420,"elapsed":23,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def create_embedding(filename, max_vocab=10000):\n","\n","    count = 0\n","    weight_list=[]\n","\n","    vocab={\"[MASK]\":0, \"[UNK]\":1}\n","\n","    with open(filename, encoding=\"utf-8\") as file:\n","        for line in file:\n","            cols=line.split()\n","\n","            if len(cols) <= 3:\n","                continue\n","\n","            vocab[cols[0]]=len(vocab)\n","\n","            w = [float(w_v) for w_v in cols[1:]]\n","            weight_list.append(w)\n","            # print(weight_list)\n","\n","            count += 1\n","            if count >= max_vocab:\n","                break\n","\n","    return vocab, weight_list"],"metadata":{"id":"nly2XDFNkX01","executionInfo":{"status":"ok","timestamp":1696826053907,"user_tz":420,"elapsed":21,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/dbamman/anlp23/main/data/glove.6B.50d.50K.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8riR9H066dPc","executionInfo":{"status":"ok","timestamp":1696826054149,"user_tz":420,"elapsed":263,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"15deceed-e3ed-42ba-cc78-4f433a72d74c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-09 04:34:13--  https://raw.githubusercontent.com/dbamman/anlp23/main/data/glove.6B.50d.50K.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21357798 (20M) [text/plain]\n","Saving to: ‘glove.6B.50d.50K.txt.3’\n","\n","glove.6B.50d.50K.tx 100%[===================>]  20.37M  --.-KB/s    in 0.09s   \n","\n","2023-10-09 04:34:14 (216 MB/s) - ‘glove.6B.50d.50K.txt.3’ saved [21357798/21357798]\n","\n"]}]},{"cell_type":"code","source":["def read_data_w_embed_vocab(filename, vocab, labels, max_length, max_docs=5000):\n","    # Read in data from file, up to the first max_docs documents. For each document\n","    # read up to max_length tokens.\n","\n","    x=[]\n","    y=[]\n","    m=[]\n","\n","    with open(filename, encoding=\"utf-8\") as file:\n","        for idx, line in enumerate(file):\n","            if idx >= max_docs:\n","                break\n","            cols=line.rstrip().split(\"\\t\")\n","            lab=cols[0]\n","            text=word_tokenize(cols[1])\n","            text_ids=[]\n","            for tok in text:\n","                if tok in vocab:\n","                    text_ids.append(vocab[tok])\n","                else:\n","                    text_ids.append(vocab[\"[UNK]\"])\n","\n","            text_ids=text_ids[:max_length]\n","\n","            # PyTorch (and most libraries that deal with matrix operations) expects all inputs to be the same length\n","            # So pad each document with 0s up to max_length\n","            # But keep track of the true number of tokens in the document with the \"mask\" list.\n","\n","            # True tokens have a mask value of 0\n","            mask=[0]*len(text_ids)\n","\n","            for i in range(len(text_ids), max_length):\n","                text_ids.append(vocab[\"[MASK]\"])\n","                # Padded tokens have a mask value of 1\n","                mask.append(1)\n","\n","            x.append(text_ids)\n","            m.append(mask)\n","            y.append(labels[lab])\n","\n","    return x, y, m"],"metadata":{"id":"5qLLDWUMkYfm","executionInfo":{"status":"ok","timestamp":1696826054341,"user_tz":420,"elapsed":194,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["vocab, weight_list = create_embedding(\"glove.6B.50d.50K.txt\")\n","train_x, train_y, train_m=read_data_w_embed_vocab(\"train.tsv\", vocab, labels, max_length=max_length)\n","dev_x, dev_y, dev_m=read_data_w_embed_vocab(\"dev.tsv\", vocab, labels, max_length=max_length)"],"metadata":{"id":"9FQMS3rg-z7t","executionInfo":{"status":"ok","timestamp":1696826060893,"user_tz":420,"elapsed":6555,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# len(weight_list), len(weight_list[0])"],"metadata":{"id":"EZ4eE7s2D8T5","executionInfo":{"status":"ok","timestamp":1696826060893,"user_tz":420,"elapsed":26,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# len(train_x), len(train_x[0])"],"metadata":{"id":"OZ5u6PVUE0-m","executionInfo":{"status":"ok","timestamp":1696826060894,"user_tz":420,"elapsed":26,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# len(train_y)"],"metadata":{"id":"Z964CfAyFqAr","executionInfo":{"status":"ok","timestamp":1696826060894,"user_tz":420,"elapsed":26,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# len(train_m), len(train_m[0])"],"metadata":{"id":"fumzLDVHFJl9","executionInfo":{"status":"ok","timestamp":1696826060894,"user_tz":420,"elapsed":25,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# len(vocab)"],"metadata":{"id":"Th9JP-gRFfUC","executionInfo":{"status":"ok","timestamp":1696826060894,"user_tz":420,"elapsed":25,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["classifier=NewTransformerClassifier(num_labels=len(labels), d_model=50, dim_feedforward=1024, weight_list=weight_list)\n","classifier=classifier.to(device)\n","\n","train_x_batch, train_y_match, train_m_match=get_batches(train_x, train_y, train_m, batch_size=batch_size)\n","dev_x_batch, dev_y_match, dev_m_match=get_batches(dev_x, dev_y, dev_m, batch_size=batch_size)\n","\n","train(classifier, \"test.model\", train_x_batch, train_y_match, train_m_match, dev_x_batch, dev_y_match, dev_m_match)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B05-wa-I_SLZ","executionInfo":{"status":"ok","timestamp":1696826069492,"user_tz":420,"elapsed":8622,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"4332edda-92a4-4122-e932-0c169265d3a5"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["0.494 is better than 0.000, saving model ...\n","Epoch 0, dev accuracy: 0.494\n","Epoch 1, dev accuracy: 0.494\n","Epoch 2, dev accuracy: 0.494\n","Epoch 3, dev accuracy: 0.494\n","Epoch 4, dev accuracy: 0.494\n","Epoch 5, dev accuracy: 0.494\n","Epoch 6, dev accuracy: 0.494\n","Epoch 7, dev accuracy: 0.494\n","Epoch 8, dev accuracy: 0.494\n","Epoch 9, dev accuracy: 0.494\n","Epoch 10, dev accuracy: 0.494\n","Epoch 11, dev accuracy: 0.494\n","11 > patience (10), stopping...\n","\n","Best Performing Model achieves dev accuracy of : 0.494\n"]}]},{"cell_type":"markdown","source":["To turn in:\n","\n","- Go to `File > Download > Download .ipynb` and save your notebook.\n","- In your browser, print this page to save as PDF.\n","- Upload both your .ipynb and .pdf files to bCourses as usual."],"metadata":{"id":"OAYpmHO92DR9"}},{"cell_type":"code","source":[],"metadata":{"id":"Iqoj5aUDrEsJ","executionInfo":{"status":"ok","timestamp":1696826069492,"user_tz":420,"elapsed":26,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"execution_count":27,"outputs":[]}]}