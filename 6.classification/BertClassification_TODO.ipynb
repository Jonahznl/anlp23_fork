{"cells":[{"cell_type":"markdown","metadata":{"id":"sDh-kfcnxKPI"},"source":["Thie notebook explores using BERT for text classification.  Before starting, change the runtime to GPU: Runtime > Change runtime type > Hardware accelerator: GPU."]},{"cell_type":"markdown","metadata":{"id":"3m2nlYY_dnXP"},"source":["First, create a folder named ANLP23_data in your Google drive account, and copy this notebook to that folder, along with `data/convote`, `data/loc` and `data/lmrd` from the Github repo.  Double click on this notebook from your drive account, which will open it in the Google Colab environment.  Begin executing the cells from that environment.\n","\n","Let's give this notebook access to the data in your ANLP23 folder so we can train and evaluate BERT on the `convote` data.  (Note you are only providing this access to yourself as you execute this notebook.)  You can give Colab notebooks access to other data in the same way (by uploading it first to your Drive account, and then providing access here)."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"p4nx0eL5cuTd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696910475485,"user_tz":420,"elapsed":847,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"38f0e480-c731-45f5-d79a-abe163ee13b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kRyQBPDKxczl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696910486367,"user_tz":420,"elapsed":10884,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"8e9a51f6-5f2a-4727-9848-701112c10fe5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mWZy26Ld0nGo","executionInfo":{"status":"ok","timestamp":1696910489986,"user_tz":420,"elapsed":3623,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["from transformers import BertModel, BertTokenizer\n","import torch\n","from tqdm import tqdm\n","import torch.nn as nn\n","import numpy as np\n","import random\n","import time"]},{"cell_type":"markdown","metadata":{"id":"g_N5SRVPyvDt"},"source":["Double-check that this notebook is running on the GPU (this should \"Running on cuda\")."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QTsJHWfVzS6Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696910489986,"user_tz":420,"elapsed":4,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"869c1eff-e74c-4206-cf13-a44f7b14c5d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running on cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Running on {}\".format(device))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"iH5KcMBMxKPP","executionInfo":{"status":"ok","timestamp":1696910489986,"user_tz":420,"elapsed":3,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["def read_labels(filename):\n","    labels={}\n","    with open(filename) as file:\n","        for line in file:\n","            cols = line.split(\"\\t\")\n","            label = cols[0]\n","            if label not in labels:\n","                labels[label]=len(labels)\n","    return labels"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZUOIgxLrxKPQ","executionInfo":{"status":"ok","timestamp":1696910489986,"user_tz":420,"elapsed":2,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["def read_data(filename, labels, max_data_points=None):\n","    \"\"\"\n","    :param filename: the name of the file\n","    :return: list of tuple ([word index list], label)\n","    as input for the forward and backward function\n","    \"\"\"\n","    data = []\n","    data_labels = []\n","    with open(filename) as file:\n","        for line in file:\n","            cols = line.split(\"\\t\")\n","            label = cols[0]\n","            text = cols[1]\n","\n","            data.append(text)\n","            data_labels.append(labels[label])\n","\n","\n","    # shuffle the data\n","    tmp = list(zip(data, data_labels))\n","    random.shuffle(tmp)\n","    data, data_labels = zip(*tmp)\n","\n","    if max_data_points is None:\n","        return data, data_labels\n","\n","    return data[:max_data_points], data_labels[:max_data_points]"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eSMOvzrNxKPS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696910490200,"user_tz":420,"elapsed":216,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"23123309-6570-41f7-dc0d-c3ab1410aa72"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'D': 0, 'R': 1}\n"]}],"source":["labels=read_labels(\"/content/drive/MyDrive/ANLP23_data/convote/train.tsv\")\n","print(labels)"]},{"cell_type":"markdown","metadata":{"id":"nvQKP6t2xKPS"},"source":["We'll limit the training and dev data to 1,000 data points for this exercise."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"JmHGHDJDxKPS","executionInfo":{"status":"ok","timestamp":1696910490201,"user_tz":420,"elapsed":5,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["train_x, train_y=read_data(\"/content/drive/MyDrive/ANLP23_data/convote/train.tsv\", labels, max_data_points=1000)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Lri3K6TsxKPT","executionInfo":{"status":"ok","timestamp":1696910490201,"user_tz":420,"elapsed":4,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["dev_x, dev_y=read_data(\"/content/drive/MyDrive/ANLP23_data/convote/dev.tsv\", labels, max_data_points=1000)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"0aMsvhvkxKPT","executionInfo":{"status":"ok","timestamp":1696910490375,"user_tz":420,"elapsed":178,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["def evaluate(model, all_x, all_y):\n","    model.eval()\n","    corr = 0.\n","    total = 0.\n","    with torch.no_grad():\n","        idx=0\n","        for x, y in zip(all_x, all_y):\n","\n","            idx+=1\n","            y_preds=model.forward(x)\n","            for idx, y_pred in enumerate(y_preds):\n","                prediction=torch.argmax(y_pred)\n","                if prediction == y[idx]:\n","                    corr += 1.\n","                total+=1\n","    return corr/total"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"uyE_hJzpxKPU","executionInfo":{"status":"ok","timestamp":1696910490375,"user_tz":420,"elapsed":3,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","\n","    def __init__(self, params):\n","        super().__init__()\n","\n","        self.model_name=params[\"model_name\"]\n","        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"], do_basic_tokenize=False)\n","        self.bert = BertModel.from_pretrained(self.model_name)\n","\n","        self.num_labels = params[\"label_length\"]\n","\n","        self.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n","\n","    def get_batches(self, all_x, all_y, batch_size=32, max_toks=256):\n","\n","        \"\"\" Get batches for input x, y data, with data tokenized according to the BERT tokenizer\n","      (and limited to a maximum number of WordPiece tokens \"\"\"\n","\n","        batches_x=[]\n","        batches_y=[]\n","\n","        for i in range(0, len(all_x), batch_size):\n","\n","            current_batch=[]\n","\n","            x=all_x[i:i+batch_size]\n","\n","            batch_x = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks)\n","            batch_y=all_y[i:i+batch_size]\n","\n","            batches_x.append(batch_x.to(device))\n","            batches_y.append(torch.LongTensor(batch_y).to(device))\n","\n","        return batches_x, batches_y\n","\n","\n","    def forward(self, batch_x):\n","\n","        bert_output = self.bert(input_ids=batch_x[\"input_ids\"],\n","                         attention_mask=batch_x[\"attention_mask\"],\n","                         token_type_ids=batch_x[\"token_type_ids\"],\n","                         output_hidden_states=True)\n","\n","        bert_hidden_states = bert_output['hidden_states']\n","\n","        # We're going to represent an entire document just by its [CLS] embedding (at position 0)\n","        out = bert_hidden_states[-1][:,0,:]\n","\n","        out = self.fc(out)\n","\n","        return out#.squeeze()"]},{"cell_type":"markdown","metadata":{"id":"EC3ysAoFfX4n"},"source":["Now let's train BERT on this data.  A few practicalities of this environment: if you encounter an out of memory error:\n","\n","* Reset the notebook (Runtime > Factory reset runtime) and execute all cells from the beginning.\n","* If your `max_length` is high, try reducing the `batch_size` in `get_batches` above.\n","\n","Even on a GPU, BERT can take a long time to train, so you might try experimenting first with smaller `max_data_points` above. before running it on the full training data."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"WZtoU7jzxKPU","executionInfo":{"status":"ok","timestamp":1696910490376,"user_tz":420,"elapsed":3,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}}},"outputs":[],"source":["def train_and_evaluate(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=None):\n","\n","  start_time=time.time()\n","  bert_model = BERTClassifier(params={\"doLowerCase\": doLowerCase, \"model_name\": bert_model_name, \"embedding_size\":embedding_size, \"label_length\": len(labels)})\n","  bert_model.to(device)\n","\n","  batch_x, batch_y = bert_model.get_batches(train_x, train_y)\n","  dev_batch_x, dev_batch_y = bert_model.get_batches(dev_x, dev_y)\n","\n","  optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n","  cross_entropy=nn.CrossEntropyLoss()\n","\n","  num_epochs=5\n","  best_dev_acc = 0.\n","\n","  for epoch in range(num_epochs):\n","      bert_model.train()\n","\n","      # Train\n","      for x, y in tqdm(list(zip(batch_x, batch_y))):\n","          y_pred = bert_model.forward(x)\n","          loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))\n","          optimizer.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","\n","      # Evaluate\n","      dev_accuracy=evaluate(bert_model, dev_batch_x, dev_batch_y)\n","      if epoch % 1 == 0:\n","          print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n","          if dev_accuracy > best_dev_acc:\n","              torch.save(bert_model.state_dict(), model_filename)\n","              best_dev_acc = dev_accuracy\n","\n","  bert_model.load_state_dict(torch.load(model_filename))\n","  print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n","  print(\"Time: %.3f seconds ---\" % (time.time() - start_time))\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"CgLvw1VRiqR_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696910727181,"user_tz":420,"elapsed":236808,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"f7dc6274-e8a4-4d5b-ae85-b1a638c6daf5"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:41<00:00,  1.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0, dev accuracy: 0.553\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:38<00:00,  1.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, dev accuracy: 0.588\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:39<00:00,  1.24s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, dev accuracy: 0.607\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:39<00:00,  1.23s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, dev accuracy: 0.646\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:39<00:00,  1.23s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, dev accuracy: 0.665\n","\n","Best Performing Model achieves dev accuracy of : 0.665\n","Time: 236.782 seconds ---\n"]}],"source":["train_and_evaluate(\"bert-base-cased\", \"convote-bert-base-cased\", train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=False)"]},{"cell_type":"markdown","metadata":{"id":"SpXSzKa8ghA4"},"source":["**Q1**.  Train BERT on your classification data (from the FeatureExploration_TODO.ipynb exercise) by following the steps outlined above.  At this point you have evaluated a number of different classification methods for that data.  Report those development accuracies below.  Be sure to enable a fair comparison by training and evaluating each method on **the same amount of data**.  Provide a one-sentence short description that details the essentials of each method (e.g., major feature classes, etc.), and tell us what data you're running these models on (either one of the three choices above, or your own data).\n","\n","|Method|Short description|Accuracy|\n","|--|--|--|\n","|Majority class|Because 1 is the majority of training data, this model choose 1 as the default answer for all dev data.|0.471|\n","|Logistic regression (6.classification/FeatureExploration.ipynb)|This logistic regression uses unigram and a few selected adjectives words as features.|0.888|\n","|BERT|This is a bert-base model, larger and taking longer time to compute than the logistic regression model.|0.842|\n","\n"]},{"cell_type":"code","source":["np.unique(np.array(train_y), return_counts=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rz5vROEoj9zy","executionInfo":{"status":"ok","timestamp":1696910727181,"user_tz":420,"elapsed":28,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"91eb20ef-c13a-4d9d-9967-d790c0aa4d7d"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0, 1]), array([465, 535]))"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["np.unique(np.array(dev_y), return_counts=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yuxOm82ijuyJ","executionInfo":{"status":"ok","timestamp":1696910727182,"user_tz":420,"elapsed":10,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"d9179c22-4db6-4aaf-803d-13b2db09dea0"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0, 1]), array([127, 130]))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["471/(529+471)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VAuHJd0thMBL","executionInfo":{"status":"ok","timestamp":1696910727182,"user_tz":420,"elapsed":9,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"b91270cf-9a54-43e6-9361-4fc3ea192555"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.471"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["labels=read_labels(\"/content/drive/MyDrive/ANLP23_data/lmrd/train.tsv\")\n","print(labels)\n","train_x, train_y=read_data(\"/content/drive/MyDrive/ANLP23_data/lmrd/train.tsv\", labels, max_data_points=1000)\n","dev_x, dev_y=read_data(\"/content/drive/MyDrive/ANLP23_data/lmrd/dev.tsv\", labels, max_data_points=1000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_c3YYkpklmPa","executionInfo":{"status":"ok","timestamp":1696910727813,"user_tz":420,"elapsed":639,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"ab00490b-fcd0-4f97-b711-8b15493de59c"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["{'pos': 0, 'neg': 1}\n"]}]},{"cell_type":"code","source":["train_and_evaluate(\"bert-base-cased\", \"lmrd-bert-base-cased\", train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrYms8rtmj2I","executionInfo":{"status":"ok","timestamp":1696911008748,"user_tz":420,"elapsed":280939,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"cb559d40-6c65-47da-d432-0fd13071822f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:40<00:00,  1.26s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0, dev accuracy: 0.775\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:39<00:00,  1.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, dev accuracy: 0.864\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:40<00:00,  1.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, dev accuracy: 0.876\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:39<00:00,  1.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, dev accuracy: 0.872\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:40<00:00,  1.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, dev accuracy: 0.842\n","\n","Best Performing Model achieves dev accuracy of : 0.876\n","Time: 281.003 seconds ---\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZB7gPBAMlWeX"},"source":["**Q2.** As you can see, training `bert-base` can be expensive.  Google has released a number of [smaller BERT models](https://github.com/google-research/bert) with fewer layers (2, 4, 6, 8, 10) and smaller dimensions (128, 256, 512) that effectively trade off accuracy for speed.  Select three of these models and train them; report both their accuracy and training time.\n","\n","\n","\n","To use these models in the huggingface library that we have been using, the huggingface name of the model can be derived from the URL linking to it:\n","\n","https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip -> `google/bert_uncased_L-2_H-128_A-2`\n","\n","All of these smaller models are uncased (so all text is lowercase), so be sure to set `doLowerCase` to be true.  You'll also need to change the `embedding_size` parameter to this function based on the H value from the model (listed both on the BERT Github page and in the model's URL).  One sample model is provided below."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"c1xQgyJs56En","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696911015655,"user_tz":420,"elapsed":6926,"user":{"displayName":"Jonah Lin","userId":"07150942445035648152"}},"outputId":"1ea3c161-4db7-45d8-9e05-4cdca57e405d"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:00<00:00, 53.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0, dev accuracy: 0.457\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:00<00:00, 62.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, dev accuracy: 0.493\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:00<00:00, 62.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, dev accuracy: 0.531\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:00<00:00, 62.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, dev accuracy: 0.566\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:00<00:00, 61.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, dev accuracy: 0.596\n","\n","Best Performing Model achieves dev accuracy of : 0.596\n","Time: 6.794 seconds ---\n"]}],"source":["train_and_evaluate(\"google/bert_uncased_L-2_H-128_A-2\", \"lmrd-uncased_L-2_H-128_A-2\", train_x, train_y, dev_x, dev_y, labels, embedding_size=128, doLowerCase=True)"]},{"cell_type":"markdown","source":["To turn in:\n","\n","- Go to `File > Download > Download .ipynb` and save your notebook.\n","- In your browser, print this page to save as PDF.\n","- Upload both your .ipynb and .pdf files to bCourses as usual."],"metadata":{"id":"KKAfd-ARQLt2"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}